{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# NYC Data Analysis: Dataset Join Feasibility Assessment\n",
    "\n",
    "This notebook examines the feasibility of joining NYC 311 service requests data with capital projects data:\n",
    "1. **Temporal dimension** - checking for time overlap\n",
    "2. **Geographic dimension** - checking for common geography\n",
    "3. **Possible join keys** - identifying common fields for data linking\n",
    "\n",
    "## Data Structure\n",
    "- `311-service-requests-from-2010-to-present.csv` - citizen service requests\n",
    "- `capital-project-schedules-and-budgets.csv` - capital construction projects\n",
    "- `311-web-content-services.csv` - web service content\n",
    "- Data dictionaries and metadata files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "plt.style.use('default')\n",
    "\n",
    "# Path to data folder\n",
    "data_path = './data/'\n",
    "print(\"Available files in data folder:\")\n",
    "for file in os.listdir(data_path):\n",
    "    print(f\"- {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1. Data Files Structure Overview\n",
    "\n",
    "First, let's load the main datasets and examine their structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load main datasets\n",
    "print(\"=== DATA LOADING ===\\n\")\n",
    "\n",
    "# 1. 311 Service Requests (main dataset)\n",
    "print(\"1. Loading 311-service-requests...\")\n",
    "try:\n",
    "    # Load first 100,000 rows for quick analysis\n",
    "    df_311 = pd.read_csv(data_path + '311-service-requests-from-2010-to-present.csv',\n",
    "                         nrows=100000, low_memory=False)\n",
    "    print(f\"   Size: {df_311.shape[0]:,} rows, {df_311.shape[1]} columns\")\n",
    "    print(f\"   Columns: {list(df_311.columns[:10])}{'...' if len(df_311.columns) > 10 else ''}\")\n",
    "except Exception as e:\n",
    "    print(f\"   Error: {e}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# 2. Capital Projects\n",
    "print(\"2. Loading capital-project-schedules...\")\n",
    "try:\n",
    "    df_capital = pd.read_csv(data_path + 'capital-project-schedules-and-budgets.csv', low_memory=False)\n",
    "    print(f\"   Size: {df_capital.shape[0]:,} rows, {df_capital.shape[1]} columns\")\n",
    "    print(f\"   Columns: {list(df_capital.columns[:10])}{'...' if len(df_capital.columns) > 10 else ''}\")\n",
    "except Exception as e:\n",
    "    print(f\"   Error: {e}\")\n",
    "    \n",
    "print()\n",
    "\n",
    "# 3. Web Content Services\n",
    "print(\"3. Loading 311-web-content-services...\")\n",
    "try:\n",
    "    df_web = pd.read_csv(data_path + '311-web-content-services.csv', low_memory=False)\n",
    "    print(f\"   Size: {df_web.shape[0]:,} rows, {df_web.shape[1]} columns\")\n",
    "    print(f\"   Columns: {list(df_web.columns)}\")\n",
    "except Exception as e:\n",
    "    print(f\"   Error: {e}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# 4. Apartment Cost List\n",
    "print(\"3. Loading apartment_cost_list...\")\n",
    "try:\n",
    "    df_housing = pd.read_csv(data_path + 'apartment_cost_list.csv', low_memory=False)\n",
    "    print(f\"   Size: {df_housing.shape[0]:,} rows, {df_housing.shape[1]} columns\")\n",
    "    print(f\"   Columns: {list(df_housing.columns)}\")\n",
    "except Exception as e:\n",
    "    print(f\"   Error: {e}\")\n",
    "\n",
    "# 4. Affordable Housing List\n",
    "print(\"3. Loading apartment_cost_list...\")\n",
    "try:\n",
    "    df_af_housing = pd.read_csv(data_path + 'affordable_housing.csv', low_memory=False)\n",
    "    print(f\"   Size: {df_af_housing.shape[0]:,} rows, {df_af_housing.shape[1]} columns\")\n",
    "    print(f\"   Columns: {list(df_af_housing.columns)}\")\n",
    "except Exception as e:\n",
    "    print(f\"   Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed structure overview of each dataset\n",
    "print(\"=== DETAILED STRUCTURE OVERVIEW ===\\n\")\n",
    "\n",
    "print(\"üìä 1. 311 SERVICE REQUESTS DATASET:\")\n",
    "print(\"=\"*50)\n",
    "print(\"Main columns:\", df_311.columns.tolist())\n",
    "print(f\"\\nData information:\")\n",
    "print(df_311.info())\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä 2. CAPITAL PROJECTS DATASET:\")\n",
    "print(\"=\"*50)\n",
    "print(\"Main columns:\", df_capital.columns.tolist())\n",
    "print(f\"\\nData information:\")\n",
    "print(df_capital.info())\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä 3. WEB CONTENT SERVICES DATASET:\")\n",
    "print(\"=\"*50)\n",
    "print(\"Main columns:\", df_web.columns.tolist())\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "print(df_web.head(3))\n",
    "\n",
    "# Detailed structure overview of each dataset\n",
    "print(\"=== DETAILED STRUCTURE OVERVIEW ===\\n\")\n",
    "\n",
    "print(\"üìä 1. 311 SERVICE REQUESTS DATASET:\")\n",
    "print(\"=\"*50)\n",
    "print(\"Main columns:\", df_311.columns.tolist())\n",
    "print(f\"\\nData information:\")\n",
    "print(df_311.info())\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä 2. CAPITAL PROJECTS DATASET:\")\n",
    "print(\"=\"*50)\n",
    "print(\"Main columns:\", df_capital.columns.tolist())\n",
    "print(f\"\\nData information:\")\n",
    "print(df_capital.info())\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä 3. WEB CONTENT SERVICES DATASET:\")\n",
    "print(\"=\"*50)\n",
    "print(\"Main columns:\", df_web.columns.tolist())\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "print(df_web.head(3))\n",
    "\n",
    "# Detailed structure overview of each dataset\n",
    "print(\"=== DETAILED STRUCTURE OVERVIEW ===\\n\")\n",
    "\n",
    "print(\"üìä 1. Apartment Cost List:\")\n",
    "print(\"=\"*50)\n",
    "print(\"Main columns:\", df_housing.columns.tolist())\n",
    "print(f\"\\nData information:\")\n",
    "print(df_housing.info())\n",
    "print(df_housing.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 2. Temporal Dimension Analysis (Time Overlap Analysis)\n",
    "\n",
    "Let's check if there's time overlap between 311 service requests and capital projects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of temporal columns in 311 and Capital Projects datasets\n",
    "print(\"=== TEMPORAL DATA ANALYSIS ===\\n\")\n",
    "\n",
    "# 1. Analysis of 311 dataset\n",
    "print(\"üìÖ 1. 311 SERVICE REQUESTS DATASET - Temporal columns:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Find all date columns\n",
    "date_columns_311 = [col for col in df_311.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "print(f\"Date columns: {date_columns_311}\")\n",
    "\n",
    "# Convert dates and analyze periods\n",
    "df_311['Created Date'] = pd.to_datetime(df_311['Created Date'], errors='coerce')\n",
    "df_311['Closed Date'] = pd.to_datetime(df_311['Closed Date'], errors='coerce')\n",
    "\n",
    "print(f\"\\n311 data period:\")\n",
    "print(f\"  Earliest creation date: {df_311['Created Date'].min()}\")\n",
    "print(f\"  Latest creation date: {df_311['Created Date'].max()}\")\n",
    "print(f\"  Earliest closure date: {df_311['Closed Date'].min()}\")\n",
    "print(f\"  Latest closure date: {df_311['Closed Date'].max()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# 2. Analysis of Capital Projects dataset  \n",
    "print(\"üìÖ 2. CAPITAL PROJECTS DATASET - Temporal columns:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Find all date columns\n",
    "date_columns_capital = [col for col in df_capital.columns if 'date' in col.lower()]\n",
    "print(f\"Date columns: {date_columns_capital}\")\n",
    "\n",
    "# Convert dates\n",
    "for date_col in date_columns_capital:\n",
    "    df_capital[date_col] = pd.to_datetime(df_capital[date_col], errors='coerce')\n",
    "    print(f\"\\n{date_col}:\")\n",
    "    print(f\"  Min: {df_capital[date_col].min()}\")\n",
    "    print(f\"  Max: {df_capital[date_col].max()}\")\n",
    "    print(f\"  Number of non-null values: {df_capital[date_col].notna().sum()}/{len(df_capital)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# 3. Analysis of Affordable Housing dataset  \n",
    "print(\"üìÖ 3. Affordable Housing DATASET - Temporal columns:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Find all date columns\n",
    "date_aff_housing = [col for col in df_af_housing.columns if 'date' in col.lower()]\n",
    "print(f\"Date columns: {date_aff_housing}\")\n",
    "\n",
    "# Convert dates\n",
    "for date_col in date_aff_housing:\n",
    "    df_af_housing[date_col] = pd.to_datetime(df_af_housing[date_col], errors='coerce')\n",
    "    print(f\"\\n{date_col}:\")\n",
    "    print(f\"  Min: {df_af_housing[date_col].min()}\")\n",
    "    print(f\"  Max: {df_af_housing[date_col].max()}\")\n",
    "    print(f\"  Number of non-null values: {df_af_housing[date_col].notna().sum()}/{len(df_af_housing)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of temporal period overlap\n",
    "print(\"\\n=== PERIOD OVERLAP ANALYSIS ===\")\n",
    "\n",
    "# Define periods for each dataset\n",
    "print(\"\\nüîç Temporal period comparison:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 311 period (from our sample)\n",
    "period_311_start = df_311['Created Date'].min()\n",
    "period_311_end = df_311['Created Date'].max()\n",
    "print(f\"311 Service Requests (sample): {period_311_start.date()} - {period_311_end.date()}\")\n",
    "\n",
    "# Capital Projects period\n",
    "period_capital_start = df_capital['Project Phase Actual Start Date'].min()\n",
    "period_capital_end = df_capital['Project Phase Actual Start Date'].max()\n",
    "print(f\"Capital Projects (start dates): {period_capital_start.date()} - {period_capital_end.date()}\")\n",
    "\n",
    "# Check for overlap\n",
    "overlap_start = max(period_311_start, period_capital_start)\n",
    "overlap_end = min(period_311_end, period_capital_end)\n",
    "\n",
    "print(f\"\\n‚úÖ OVERLAP ANALYSIS RESULT:\")\n",
    "if overlap_start <= overlap_end:\n",
    "    print(f\"üéØ OVERLAP EXISTS! Period: {overlap_start.date()} - {overlap_end.date()}\")\n",
    "    overlap_days = (overlap_end - overlap_start).days\n",
    "    print(f\"üìä Overlap duration: {overlap_days} days\")\n",
    "    \n",
    "    # Count records in overlap period\n",
    "    count_311_overlap = df_311[\n",
    "        (df_311['Created Date'] >= overlap_start) & \n",
    "        (df_311['Created Date'] <= overlap_end)\n",
    "    ].shape[0]\n",
    "    \n",
    "    count_capital_overlap = df_capital[\n",
    "        (df_capital['Project Phase Actual Start Date'] >= overlap_start) & \n",
    "        (df_capital['Project Phase Actual Start Date'] <= overlap_end)\n",
    "    ].shape[0]\n",
    "    \n",
    "    print(f\"üìà 311 requests in overlap period: {count_311_overlap:,}\")\n",
    "    print(f\"üìà Capital projects (start) in period: {count_capital_overlap:,}\")\n",
    "else:\n",
    "    print(\"‚ùå NO OVERLAP\")\n",
    "\n",
    "# Also check with all capital project dates\n",
    "print(f\"\\nüîÑ Additional analysis with all capital project dates:\")\n",
    "capital_all_dates = pd.concat([\n",
    "    df_capital['Project Phase Actual Start Date'].dropna(),\n",
    "    df_capital['Project Phase Planned End Date'].dropna(),\n",
    "    df_capital['Project Phase Actual End Date'].dropna()\n",
    "])\n",
    "\n",
    "capital_min_all = capital_all_dates.min()\n",
    "capital_max_all = capital_all_dates.max()\n",
    "print(f\"Full capital projects period: {capital_min_all.date()} - {capital_max_all.date()}\")\n",
    "\n",
    "overlap_start_all = max(period_311_start, capital_min_all)\n",
    "overlap_end_all = min(period_311_end, capital_max_all)\n",
    "\n",
    "if overlap_start_all <= overlap_end_all:\n",
    "    print(f\"‚úÖ Overlap with all dates: {overlap_start_all.date()} - {overlap_end_all.date()}\")\n",
    "    print(f\"üìä Duration: {(overlap_end_all - overlap_start_all).days} days\")\n",
    "else:\n",
    "    print(\"‚ùå No overlap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 3. Geographic Data Analysis (Spatial Analysis)\n",
    "\n",
    "Let's check if there are common geographic identifiers for spatial joining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of geographic columns in datasets\n",
    "print(\"=== GEOGRAPHIC DATA ANALYSIS ===\\n\")\n",
    "\n",
    "# 1. Analysis of geographic columns in 311 dataset\n",
    "print(\"üìç 1. 311 SERVICE REQUESTS DATASET - Geographic columns:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Find columns with geographic data\n",
    "geo_keywords = ['location', 'address', 'borough', 'zip', 'latitude', 'longitude', 'district', 'community']\n",
    "geo_columns_311 = [col for col in df_311.columns \n",
    "                   if any(keyword in col.lower() for keyword in geo_keywords)]\n",
    "\n",
    "print(f\"Geographic columns: {geo_columns_311}\")\n",
    "\n",
    "# Analyze key geographic fields\n",
    "key_geo_fields_311 = ['Borough', 'Incident Zip', 'Latitude', 'Longitude', 'Community Board']\n",
    "for field in key_geo_fields_311:\n",
    "    if field in df_311.columns:\n",
    "        unique_count = df_311[field].nunique()\n",
    "        null_count = df_311[field].isnull().sum()\n",
    "        print(f\"\\n{field}:\")\n",
    "        print(f\"  Unique values: {unique_count}\")\n",
    "        print(f\"  Missing values: {null_count}/{len(df_311)} ({null_count/len(df_311)*100:.1f}%)\")\n",
    "        if unique_count < 20:  # Show values if not too many\n",
    "            print(f\"  Values: {sorted(df_311[field].dropna().unique())}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# 2. Analysis of geographic columns in Capital Projects dataset\n",
    "print(\"üìç 2. CAPITAL PROJECTS DATASET - Geographic columns:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "geo_columns_capital = [col for col in df_capital.columns \n",
    "                      if any(keyword in col.lower() for keyword in geo_keywords)]\n",
    "print(f\"Geographic columns: {geo_columns_capital}\")\n",
    "\n",
    "# Analyze key geographic fields\n",
    "key_geo_fields_capital = ['Project Geographic District ', 'Project School Name']\n",
    "for field in key_geo_fields_capital:\n",
    "    if field in df_capital.columns:\n",
    "        unique_count = df_capital[field].nunique()\n",
    "        null_count = df_capital[field].isnull().sum()\n",
    "        print(f\"\\n{field}:\")\n",
    "        print(f\"  Unique values: {unique_count}\")\n",
    "        print(f\"  Missing values: {null_count}/{len(df_capital)} ({null_count/len(df_capital)*100:.1f}%)\")\n",
    "        if unique_count < 30:  # Show values if not too many\n",
    "            sample_values = df_capital[field].dropna().unique()[:10]  # First 10 values\n",
    "            print(f\"  Sample values: {list(sample_values)}\")\n",
    "\n",
    "# Specific analysis of Geographic District\n",
    "if 'Project Geographic District ' in df_capital.columns:\n",
    "    print(f\"\\nüîç Detailed analysis of Project Geographic District:\")\n",
    "    district_counts = df_capital['Project Geographic District '].value_counts()\n",
    "    print(f\"Top 10 districts by project count:\")\n",
    "    print(district_counts.head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 4. Join Keys Identification\n",
    "\n",
    "Let's analyze possible ways to join the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for possible keys to join datasets\n",
    "print(\"=== JOIN KEYS SEARCH ===\\n\")\n",
    "\n",
    "# 1. Compare all columns to find common ones\n",
    "print(\"üîç 1. COLUMN COMPARISON BETWEEN DATASETS:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "columns_311 = set(df_311.columns)\n",
    "columns_capital = set(df_capital.columns)\n",
    "\n",
    "# Search for exact matches\n",
    "exact_matches = columns_311.intersection(columns_capital)\n",
    "print(f\"Exact column matches: {list(exact_matches) if exact_matches else 'None'}\")\n",
    "\n",
    "# Search for similar columns (by name)\n",
    "similar_pairs = []\n",
    "for col_311 in columns_311:\n",
    "    for col_capital in columns_capital:\n",
    "        # Check similarity by keywords\n",
    "        keywords_common = ['district', 'borough', 'location', 'address', 'zip', 'community']\n",
    "        col_311_lower = col_311.lower()\n",
    "        col_capital_lower = col_capital.lower()\n",
    "        \n",
    "        for keyword in keywords_common:\n",
    "            if keyword in col_311_lower and keyword in col_capital_lower:\n",
    "                similar_pairs.append((col_311, col_capital, keyword))\n",
    "\n",
    "print(f\"\\nSimilar columns by keywords:\")\n",
    "for pair in similar_pairs:\n",
    "    print(f\"  311: '{pair[0]}' <-> Capital: '{pair[1]}' (common: {pair[2]})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# 2. Analysis of spatial join possibilities\n",
    "print(\"üó∫Ô∏è 2. SPATIAL JOIN POSSIBILITIES:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Check Borough in 311 and Geographic District in Capital\n",
    "if 'Borough' in df_311.columns and 'Project Geographic District ' in df_capital.columns:\n",
    "    \n",
    "    # Unique boroughs in 311\n",
    "    boroughs_311 = set(df_311['Borough'].dropna().unique())\n",
    "    print(f\"Boroughs in 311 dataset ({len(boroughs_311)}): {sorted(boroughs_311)}\")\n",
    "    \n",
    "    # Unique districts in Capital Projects\n",
    "    districts_capital = set(df_capital['Project Geographic District '].dropna().unique())\n",
    "    print(f\"\\nDistricts in Capital dataset ({len(districts_capital)}):\")\n",
    "    print(f\"First 10: {sorted(list(districts_capital))[:10]}\")\n",
    "    \n",
    "    # Try to find correspondences between Borough and District\n",
    "    print(f\"\\nüîÑ Search for Borough <-> District correspondences:\")\n",
    "    \n",
    "    # NYC Borough to district numbers mapping\n",
    "    nyc_boroughs = ['MANHATTAN', 'BROOKLYN', 'QUEENS', 'BRONX', 'STATEN ISLAND']\n",
    "    \n",
    "    for borough in boroughs_311:\n",
    "        if borough and borough.upper() in nyc_boroughs:\n",
    "            # Count records for this borough\n",
    "            count_311 = df_311[df_311['Borough'] == borough].shape[0]\n",
    "            print(f\"  {borough}: {count_311:,} 311 requests\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# 3. Community Board analysis as possible key\n",
    "print(\"üèòÔ∏è 3. COMMUNITY BOARD ANALYSIS:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if 'Community Board' in df_311.columns:\n",
    "    cb_311 = df_311['Community Board'].dropna().unique()\n",
    "    print(f\"Community Board in 311 ({len(cb_311)} unique):\")\n",
    "    print(f\"Examples: {sorted(cb_311)[:10]}\")\n",
    "    \n",
    "    # Check if there are similar fields in Capital\n",
    "    cb_like_fields = [col for col in df_capital.columns if 'board' in col.lower() or 'community' in col.lower()]\n",
    "    print(f\"\\nSimilar fields in Capital: {cb_like_fields}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# 4. Coordinates analysis\n",
    "print(\"üìç 4. COORDINATES ANALYSIS:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if 'Latitude' in df_311.columns and 'Longitude' in df_311.columns:\n",
    "    lat_count = df_311['Latitude'].notna().sum()\n",
    "    lon_count = df_311['Longitude'].notna().sum()\n",
    "    print(f\"311 dataset: {lat_count:,} records with latitude coordinates, {lon_count:,} with longitude\")\n",
    "    \n",
    "    print(f\"311 coordinate ranges:\")\n",
    "    print(f\"  Latitude: {df_311['Latitude'].min():.4f} - {df_311['Latitude'].max():.4f}\")\n",
    "    print(f\"  Longitude: {df_311['Longitude'].min():.4f} - {df_311['Longitude'].max():.4f}\")\n",
    "\n",
    "# Check if there are coordinates in Capital\n",
    "coord_fields_capital = [col for col in df_capital.columns if any(word in col.lower() for word in ['lat', 'lon', 'coord'])]\n",
    "print(f\"\\nCoordinate fields in Capital: {coord_fields_capital if coord_fields_capital else 'No explicit coordinate fields'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# 5. Summary of possible join strategies\n",
    "print(\"üí° 5. POSSIBLE JOIN STRATEGIES:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "strategies = [\n",
    "    {\n",
    "        'name': 'Geographic join via Borough/District',\n",
    "        'feasible': bool(boroughs_311 and districts_capital),\n",
    "        'description': 'Mapping Borough (311) -> Geographic District (Capital)',\n",
    "        'challenge': 'Need additional mapping between Borough and District numbers'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Spatial join via coordinates',\n",
    "        'feasible': lat_count > 0 and len(coord_fields_capital) == 0,\n",
    "        'description': 'Using 311 coordinates to determine proximity to Capital projects',\n",
    "        'challenge': 'Capital projects lack coordinates - need geocoding'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Temporal-geographic join',\n",
    "        'feasible': True,\n",
    "        'description': 'Combining temporal overlap + geographic proximity',\n",
    "        'challenge': 'Need additional geographic reference'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Join via external sources',\n",
    "        'feasible': True,\n",
    "        'description': 'Using additional NYC geographic reference data',\n",
    "        'challenge': 'Need external data for mapping'\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, strategy in enumerate(strategies, 1):\n",
    "    status = \"‚úÖ Possible\" if strategy['feasible'] else \"‚ùå Difficult\"\n",
    "    print(f\"{i}. {strategy['name']} - {status}\")\n",
    "    print(f\"   Description: {strategy['description']}\")\n",
    "    print(f\"   Challenge: {strategy['challenge']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 5. Conclusions and Recommendations\n",
    "\n",
    "Based on the conducted analysis, here are the summary conclusions regarding data joining possibilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final conclusions and recommendations\n",
    "print(\"=\" * 80)\n",
    "print(\"üìã FINAL CONCLUSIONS AND RECOMMENDATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n‚úÖ 1. TEMPORAL DIMENSION (TIME OVERLAP)\")\n",
    "print(\"-\" * 50)\n",
    "print(\"RESULT: Significant temporal overlap exists!\")\n",
    "print(f\"‚Ä¢ Overlap period: 18 days (2019-11-12 to 2019-12-01)\")\n",
    "print(f\"‚Ä¢ 311 requests in period: 100,000 records\")\n",
    "print(f\"‚Ä¢ Capital projects: 74 projects started in this period\")\n",
    "print(\"‚Ä¢ Overall Capital Projects period: 2003-2023 (covers all possible 311 periods)\")\n",
    "\n",
    "print(\"\\n‚úÖ 2. GEOGRAPHIC DIMENSION\")\n",
    "print(\"-\" * 50)\n",
    "print(\"RESULT: Geographic joining possibilities exist!\")\n",
    "print(\"‚Ä¢ 311 dataset has Borough (5 NYC boroughs) + coordinates (96,696 records)\")\n",
    "print(\"‚Ä¢ Capital Projects has Geographic District (33 districts)\")\n",
    "print(\"‚Ä¢ Community Board in 311 (76 unique districts)\")\n",
    "print(\"‚Ä¢ Coordinates exist only in 311, Capital Projects lacks them\")\n",
    "\n",
    "print(\"\\nüí° 3. RECOMMENDED JOIN STRATEGIES\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "strategies = [\n",
    "    {\n",
    "        \"priority\": \"High\",\n",
    "        \"name\": \"Borough ‚Üí District Mapping\",\n",
    "        \"description\": \"Create mapping between Borough (311) and Geographic District (Capital)\",\n",
    "        \"implementation\": \"Use NYC School Districts or Community Districts reference\",\n",
    "        \"pros\": \"Direct relationship, high accuracy\",\n",
    "        \"cons\": \"Requires additional reference data\"\n",
    "    },\n",
    "    {\n",
    "        \"priority\": \"Medium\", \n",
    "        \"name\": \"Temporal-geographic join\",\n",
    "        \"description\": \"Combine temporal overlap + geographic proximity\",\n",
    "        \"implementation\": \"Filter by time + group by Borough/District\",\n",
    "        \"pros\": \"Enables analysis of construction impact on requests\",\n",
    "        \"cons\": \"More complex logic, requires validation\"\n",
    "    },\n",
    "    {\n",
    "        \"priority\": \"Low\",\n",
    "        \"name\": \"Geocoding + spatial join\", \n",
    "        \"description\": \"Add coordinates to Capital Projects via geocoding\",\n",
    "        \"implementation\": \"Geocode project addresses, use search radius\",\n",
    "        \"pros\": \"Most accurate spatial joining\",\n",
    "        \"cons\": \"Requires geocoding, computationally intensive\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, strategy in enumerate(strategies, 1):\n",
    "    print(f\"\\n{i}. {strategy['name']} (Priority: {strategy['priority']})\")\n",
    "    print(f\"   üìù Description: {strategy['description']}\")\n",
    "    print(f\"   üî® Implementation: {strategy['implementation']}\")\n",
    "    print(f\"   ‚úÖ Advantages: {strategy['pros']}\")\n",
    "    print(f\"   ‚ö†Ô∏è  Disadvantages: {strategy['cons']}\")\n",
    "\n",
    "print(f\"\\nüéØ 4. BEST APPROACH FOR ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Recommended combined strategy:\")\n",
    "print(\"1Ô∏è‚É£ Temporal filtering: select overlap period\")\n",
    "print(\"2Ô∏è‚É£ Geographic grouping: Borough (311) + additional mapping to District\")\n",
    "print(\"3Ô∏è‚É£ Correlation analysis: requests before/during/after projects\")\n",
    "print(\"4Ô∏è‚É£ Visualization: maps with overlaid zones and time series\")\n",
    "\n",
    "print(f\"\\nüìä 5. EXPECTED ANALYSIS RESULTS\")\n",
    "print(\"-\" * 50)\n",
    "research_questions = [\n",
    "    \"Do 311 requests increase during active construction projects?\",\n",
    "    \"What types of requests are most commonly related to construction work?\", \n",
    "    \"In which districts does construction most impact citizen requests?\",\n",
    "    \"How long does the impact of construction projects on request volume last?\",\n",
    "    \"Are there seasonal or weekly patterns in the relationship between projects and requests?\"\n",
    "]\n",
    "\n",
    "for i, question in enumerate(research_questions, 1):\n",
    "    print(f\"{i}. {question}\")\n",
    "\n",
    "print(f\"\\nüìã 6. NEXT STEPS\")\n",
    "print(\"-\" * 50)\n",
    "next_steps = [\n",
    "    \"Load complete 311 dataset (not just 100K records)\",\n",
    "    \"Find or create Borough ‚Üí Geographic District mapping\",\n",
    "    \"Implement temporal-geographic join\", \n",
    "    \"Conduct exploratory analysis of joined data\",\n",
    "    \"Create visualizations to test hypotheses\",\n",
    "    \"Statistically verify correlations between projects and requests\"\n",
    "]\n",
    "\n",
    "for i, step in enumerate(next_steps, 1):\n",
    "    print(f\"{i}. {step}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python daw .venv",
   "language": "python",
   "name": "myproj-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
