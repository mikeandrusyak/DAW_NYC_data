{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# NYC Data Analysis: Dataset Join Feasibility Assessment\n",
    "\n",
    "This notebook examines the feasibility of joining NYC 311 service requests data with capital projects data:\n",
    "1. **Temporal dimension** - checking for time overlap\n",
    "2. **Geographic dimension** - checking for common geography\n",
    "3. **Possible join keys** - identifying common fields for data linking\n",
    "\n",
    "## Data Structure\n",
    "- `311-service-requests-from-2010-to-present.csv` - citizen service requests\n",
    "- `capital-project-schedules-and-budgets.csv` - capital construction projects\n",
    "- `311-web-content-services.csv` - web service content\n",
    "- Data dictionaries and metadata files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "plt.style.use('default')\n",
    "\n",
    "# Path to data folder\n",
    "data_path = './data/'\n",
    "print(\"Available files in data folder:\")\n",
    "for file in os.listdir(data_path):\n",
    "    print(f\"- {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1. Data Files Structure Overview\n",
    "\n",
    "First, let's load the main datasets and examine their structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load main datasets\n",
    "print(\"=== DATA LOADING ===\\n\")\n",
    "\n",
    "# 1. 311 Service Requests (main dataset)\n",
    "print(\"1. Loading 311-service-requests...\")\n",
    "try:\n",
    "    # Load first 100,000 rows for quick analysis\n",
    "    df_311 = pd.read_csv(data_path + '311-service-requests-from-2010-to-present.csv',\n",
    "                         nrows=100000, low_memory=False)\n",
    "    print(f\"   Size: {df_311.shape[0]:,} rows, {df_311.shape[1]} columns\")\n",
    "    print(f\"   Columns: {list(df_311.columns[:10])}{'...' if len(df_311.columns) > 10 else ''}\")\n",
    "except Exception as e:\n",
    "    print(f\"   Error: {e}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# 2. Capital Projects\n",
    "print(\"2. Loading capital-project-schedules...\")\n",
    "try:\n",
    "    df_capital = pd.read_csv(data_path + 'capital-project-schedules-and-budgets.csv', low_memory=False)\n",
    "    print(f\"   Size: {df_capital.shape[0]:,} rows, {df_capital.shape[1]} columns\")\n",
    "    print(f\"   Columns: {list(df_capital.columns[:10])}{'...' if len(df_capital.columns) > 10 else ''}\")\n",
    "except Exception as e:\n",
    "    print(f\"   Error: {e}\")\n",
    "    \n",
    "print()\n",
    "\n",
    "# 3. Web Content Services\n",
    "print(\"3. Loading 311-web-content-services...\")\n",
    "try:\n",
    "    df_web = pd.read_csv(data_path + '311-web-content-services.csv', low_memory=False)\n",
    "    print(f\"   Size: {df_web.shape[0]:,} rows, {df_web.shape[1]} columns\")\n",
    "    print(f\"   Columns: {list(df_web.columns)}\")\n",
    "except Exception as e:\n",
    "    print(f\"   Error: {e}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# 4. Apartment Cost List\n",
    "print(\"3. Loading apartment_cost_list...\")\n",
    "try:\n",
    "    df_housing = pd.read_csv(data_path + 'apartment_cost_list.csv', low_memory=False)\n",
    "    print(f\"   Size: {df_housing.shape[0]:,} rows, {df_housing.shape[1]} columns\")\n",
    "    print(f\"   Columns: {list(df_housing.columns)}\")\n",
    "except Exception as e:\n",
    "    print(f\"   Error: {e}\")\n",
    "\n",
    "# 4. Affordable Housing List\n",
    "print(\"3. Loading apartment_cost_list...\")\n",
    "try:\n",
    "    df_af_housing = pd.read_csv(data_path + 'affordable_housing.csv', low_memory=False)\n",
    "    print(f\"   Size: {df_af_housing.shape[0]:,} rows, {df_af_housing.shape[1]} columns\")\n",
    "    print(f\"   Columns: {list(df_af_housing.columns)}\")\n",
    "except Exception as e:\n",
    "    print(f\"   Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed structure overview of each dataset\n",
    "print(\"=== DETAILED STRUCTURE OVERVIEW ===\\n\")\n",
    "\n",
    "print(\"📊 1. 311 SERVICE REQUESTS DATASET:\")\n",
    "print(\"=\"*50)\n",
    "print(\"Main columns:\", df_311.columns.tolist())\n",
    "print(f\"\\nData information:\")\n",
    "print(df_311.info())\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"📊 2. CAPITAL PROJECTS DATASET:\")\n",
    "print(\"=\"*50)\n",
    "print(\"Main columns:\", df_capital.columns.tolist())\n",
    "print(f\"\\nData information:\")\n",
    "print(df_capital.info())\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"📊 3. WEB CONTENT SERVICES DATASET:\")\n",
    "print(\"=\"*50)\n",
    "print(\"Main columns:\", df_web.columns.tolist())\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "print(df_web.head(3))\n",
    "\n",
    "# Detailed structure overview of each dataset\n",
    "print(\"=== DETAILED STRUCTURE OVERVIEW ===\\n\")\n",
    "\n",
    "print(\"📊 1. 311 SERVICE REQUESTS DATASET:\")\n",
    "print(\"=\"*50)\n",
    "print(\"Main columns:\", df_311.columns.tolist())\n",
    "print(f\"\\nData information:\")\n",
    "print(df_311.info())\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"📊 2. CAPITAL PROJECTS DATASET:\")\n",
    "print(\"=\"*50)\n",
    "print(\"Main columns:\", df_capital.columns.tolist())\n",
    "print(f\"\\nData information:\")\n",
    "print(df_capital.info())\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"📊 3. WEB CONTENT SERVICES DATASET:\")\n",
    "print(\"=\"*50)\n",
    "print(\"Main columns:\", df_web.columns.tolist())\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "print(df_web.head(3))\n",
    "\n",
    "# Detailed structure overview of each dataset\n",
    "print(\"=== DETAILED STRUCTURE OVERVIEW ===\\n\")\n",
    "\n",
    "print(\"📊 1. Apartment Cost List:\")\n",
    "print(\"=\"*50)\n",
    "print(\"Main columns:\", df_housing.columns.tolist())\n",
    "print(f\"\\nData information:\")\n",
    "print(df_housing.info())\n",
    "print(df_housing.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 2. Temporal Dimension Analysis (Time Overlap Analysis)\n",
    "\n",
    "Let's check if there's time overlap between 311 service requests and capital projects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of temporal columns in 311 and Capital Projects datasets\n",
    "print(\"=== TEMPORAL DATA ANALYSIS ===\\n\")\n",
    "\n",
    "# 1. Analysis of 311 dataset\n",
    "print(\"📅 1. 311 SERVICE REQUESTS DATASET - Temporal columns:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Find all date columns\n",
    "date_columns_311 = [col for col in df_311.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "print(f\"Date columns: {date_columns_311}\")\n",
    "\n",
    "# Convert dates and analyze periods\n",
    "df_311['Created Date'] = pd.to_datetime(df_311['Created Date'], errors='coerce')\n",
    "df_311['Closed Date'] = pd.to_datetime(df_311['Closed Date'], errors='coerce')\n",
    "\n",
    "print(f\"\\n311 data period:\")\n",
    "print(f\"  Earliest creation date: {df_311['Created Date'].min()}\")\n",
    "print(f\"  Latest creation date: {df_311['Created Date'].max()}\")\n",
    "print(f\"  Earliest closure date: {df_311['Closed Date'].min()}\")\n",
    "print(f\"  Latest closure date: {df_311['Closed Date'].max()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# 2. Analysis of Capital Projects dataset  \n",
    "print(\"📅 2. CAPITAL PROJECTS DATASET - Temporal columns:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Find all date columns\n",
    "date_columns_capital = [col for col in df_capital.columns if 'date' in col.lower()]\n",
    "print(f\"Date columns: {date_columns_capital}\")\n",
    "\n",
    "# Convert dates\n",
    "for date_col in date_columns_capital:\n",
    "    df_capital[date_col] = pd.to_datetime(df_capital[date_col], errors='coerce')\n",
    "    print(f\"\\n{date_col}:\")\n",
    "    print(f\"  Min: {df_capital[date_col].min()}\")\n",
    "    print(f\"  Max: {df_capital[date_col].max()}\")\n",
    "    print(f\"  Number of non-null values: {df_capital[date_col].notna().sum()}/{len(df_capital)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# 3. Analysis of Affordable Housing dataset  \n",
    "print(\"📅 3. Affordable Housing DATASET - Temporal columns:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Find all date columns\n",
    "date_aff_housing = [col for col in df_af_housing.columns if 'date' in col.lower()]\n",
    "print(f\"Date columns: {date_aff_housing}\")\n",
    "\n",
    "# Convert dates\n",
    "for date_col in date_aff_housing:\n",
    "    df_af_housing[date_col] = pd.to_datetime(df_af_housing[date_col], errors='coerce')\n",
    "    print(f\"\\n{date_col}:\")\n",
    "    print(f\"  Min: {df_af_housing[date_col].min()}\")\n",
    "    print(f\"  Max: {df_af_housing[date_col].max()}\")\n",
    "    print(f\"  Number of non-null values: {df_af_housing[date_col].notna().sum()}/{len(df_af_housing)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of temporal period overlap\n",
    "print(\"\\n=== PERIOD OVERLAP ANALYSIS ===\")\n",
    "\n",
    "# Define periods for each dataset\n",
    "print(\"\\n🔍 Temporal period comparison:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 311 period (from our sample)\n",
    "period_311_start = df_311['Created Date'].min()\n",
    "period_311_end = df_311['Created Date'].max()\n",
    "print(f\"311 Service Requests (sample): {period_311_start.date()} - {period_311_end.date()}\")\n",
    "\n",
    "# Capital Projects period\n",
    "period_capital_start = df_capital['Project Phase Actual Start Date'].min()\n",
    "period_capital_end = df_capital['Project Phase Actual Start Date'].max()\n",
    "print(f\"Capital Projects (start dates): {period_capital_start.date()} - {period_capital_end.date()}\")\n",
    "\n",
    "# Check for overlap\n",
    "overlap_start = max(period_311_start, period_capital_start)\n",
    "overlap_end = min(period_311_end, period_capital_end)\n",
    "\n",
    "print(f\"\\n✅ OVERLAP ANALYSIS RESULT:\")\n",
    "if overlap_start <= overlap_end:\n",
    "    print(f\"🎯 OVERLAP EXISTS! Period: {overlap_start.date()} - {overlap_end.date()}\")\n",
    "    overlap_days = (overlap_end - overlap_start).days\n",
    "    print(f\"📊 Overlap duration: {overlap_days} days\")\n",
    "    \n",
    "    # Count records in overlap period\n",
    "    count_311_overlap = df_311[\n",
    "        (df_311['Created Date'] >= overlap_start) & \n",
    "        (df_311['Created Date'] <= overlap_end)\n",
    "    ].shape[0]\n",
    "    \n",
    "    count_capital_overlap = df_capital[\n",
    "        (df_capital['Project Phase Actual Start Date'] >= overlap_start) & \n",
    "        (df_capital['Project Phase Actual Start Date'] <= overlap_end)\n",
    "    ].shape[0]\n",
    "    \n",
    "    print(f\"📈 311 requests in overlap period: {count_311_overlap:,}\")\n",
    "    print(f\"📈 Capital projects (start) in period: {count_capital_overlap:,}\")\n",
    "else:\n",
    "    print(\"❌ NO OVERLAP\")\n",
    "\n",
    "# Also check with all capital project dates\n",
    "print(f\"\\n🔄 Additional analysis with all capital project dates:\")\n",
    "capital_all_dates = pd.concat([\n",
    "    df_capital['Project Phase Actual Start Date'].dropna(),\n",
    "    df_capital['Project Phase Planned End Date'].dropna(),\n",
    "    df_capital['Project Phase Actual End Date'].dropna()\n",
    "])\n",
    "\n",
    "capital_min_all = capital_all_dates.min()\n",
    "capital_max_all = capital_all_dates.max()\n",
    "print(f\"Full capital projects period: {capital_min_all.date()} - {capital_max_all.date()}\")\n",
    "\n",
    "overlap_start_all = max(period_311_start, capital_min_all)\n",
    "overlap_end_all = min(period_311_end, capital_max_all)\n",
    "\n",
    "if overlap_start_all <= overlap_end_all:\n",
    "    print(f\"✅ Overlap with all dates: {overlap_start_all.date()} - {overlap_end_all.date()}\")\n",
    "    print(f\"📊 Duration: {(overlap_end_all - overlap_start_all).days} days\")\n",
    "else:\n",
    "    print(\"❌ No overlap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 3. Geographic Data Analysis (Spatial Analysis)\n",
    "\n",
    "Let's check if there are common geographic identifiers for spatial joining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of geographic columns in datasets\n",
    "print(\"=== GEOGRAPHIC DATA ANALYSIS ===\\n\")\n",
    "\n",
    "# 1. Analysis of geographic columns in 311 dataset\n",
    "print(\"📍 1. 311 SERVICE REQUESTS DATASET - Geographic columns:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Find columns with geographic data\n",
    "geo_keywords = ['location', 'address', 'borough', 'zip', 'latitude', 'longitude', 'district', 'community']\n",
    "geo_columns_311 = [col for col in df_311.columns \n",
    "                   if any(keyword in col.lower() for keyword in geo_keywords)]\n",
    "\n",
    "print(f\"Geographic columns: {geo_columns_311}\")\n",
    "\n",
    "# Analyze key geographic fields\n",
    "key_geo_fields_311 = ['Borough', 'Incident Zip', 'Latitude', 'Longitude', 'Community Board']\n",
    "for field in key_geo_fields_311:\n",
    "    if field in df_311.columns:\n",
    "        unique_count = df_311[field].nunique()\n",
    "        null_count = df_311[field].isnull().sum()\n",
    "        print(f\"\\n{field}:\")\n",
    "        print(f\"  Unique values: {unique_count}\")\n",
    "        print(f\"  Missing values: {null_count}/{len(df_311)} ({null_count/len(df_311)*100:.1f}%)\")\n",
    "        if unique_count < 20:  # Show values if not too many\n",
    "            print(f\"  Values: {sorted(df_311[field].dropna().unique())}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# 2. Analysis of geographic columns in Capital Projects dataset\n",
    "print(\"📍 2. CAPITAL PROJECTS DATASET - Geographic columns:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "geo_columns_capital = [col for col in df_capital.columns \n",
    "                      if any(keyword in col.lower() for keyword in geo_keywords)]\n",
    "print(f\"Geographic columns: {geo_columns_capital}\")\n",
    "\n",
    "# Analyze key geographic fields\n",
    "key_geo_fields_capital = ['Project Geographic District ', 'Project School Name']\n",
    "for field in key_geo_fields_capital:\n",
    "    if field in df_capital.columns:\n",
    "        unique_count = df_capital[field].nunique()\n",
    "        null_count = df_capital[field].isnull().sum()\n",
    "        print(f\"\\n{field}:\")\n",
    "        print(f\"  Unique values: {unique_count}\")\n",
    "        print(f\"  Missing values: {null_count}/{len(df_capital)} ({null_count/len(df_capital)*100:.1f}%)\")\n",
    "        if unique_count < 30:  # Show values if not too many\n",
    "            sample_values = df_capital[field].dropna().unique()[:10]  # First 10 values\n",
    "            print(f\"  Sample values: {list(sample_values)}\")\n",
    "\n",
    "# Specific analysis of Geographic District\n",
    "if 'Project Geographic District ' in df_capital.columns:\n",
    "    print(f\"\\n🔍 Detailed analysis of Project Geographic District:\")\n",
    "    district_counts = df_capital['Project Geographic District '].value_counts()\n",
    "    print(f\"Top 10 districts by project count:\")\n",
    "    print(district_counts.head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 4. Join Keys Identification\n",
    "\n",
    "Let's analyze possible ways to join the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for possible keys to join datasets\n",
    "print(\"=== JOIN KEYS SEARCH ===\\n\")\n",
    "\n",
    "# 1. Compare all columns to find common ones\n",
    "print(\"🔍 1. COLUMN COMPARISON BETWEEN DATASETS:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "columns_311 = set(df_311.columns)\n",
    "columns_capital = set(df_capital.columns)\n",
    "\n",
    "# Search for exact matches\n",
    "exact_matches = columns_311.intersection(columns_capital)\n",
    "print(f\"Exact column matches: {list(exact_matches) if exact_matches else 'None'}\")\n",
    "\n",
    "# Search for similar columns (by name)\n",
    "similar_pairs = []\n",
    "for col_311 in columns_311:\n",
    "    for col_capital in columns_capital:\n",
    "        # Check similarity by keywords\n",
    "        keywords_common = ['district', 'borough', 'location', 'address', 'zip', 'community']\n",
    "        col_311_lower = col_311.lower()\n",
    "        col_capital_lower = col_capital.lower()\n",
    "        \n",
    "        for keyword in keywords_common:\n",
    "            if keyword in col_311_lower and keyword in col_capital_lower:\n",
    "                similar_pairs.append((col_311, col_capital, keyword))\n",
    "\n",
    "print(f\"\\nSimilar columns by keywords:\")\n",
    "for pair in similar_pairs:\n",
    "    print(f\"  311: '{pair[0]}' <-> Capital: '{pair[1]}' (common: {pair[2]})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# 2. Analysis of spatial join possibilities\n",
    "print(\"🗺️ 2. SPATIAL JOIN POSSIBILITIES:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Check Borough in 311 and Geographic District in Capital\n",
    "if 'Borough' in df_311.columns and 'Project Geographic District ' in df_capital.columns:\n",
    "    \n",
    "    # Unique boroughs in 311\n",
    "    boroughs_311 = set(df_311['Borough'].dropna().unique())\n",
    "    print(f\"Boroughs in 311 dataset ({len(boroughs_311)}): {sorted(boroughs_311)}\")\n",
    "    \n",
    "    # Unique districts in Capital Projects\n",
    "    districts_capital = set(df_capital['Project Geographic District '].dropna().unique())\n",
    "    print(f\"\\nDistricts in Capital dataset ({len(districts_capital)}):\")\n",
    "    print(f\"First 10: {sorted(list(districts_capital))[:10]}\")\n",
    "    \n",
    "    # Try to find correspondences between Borough and District\n",
    "    print(f\"\\n🔄 Search for Borough <-> District correspondences:\")\n",
    "    \n",
    "    # NYC Borough to district numbers mapping\n",
    "    nyc_boroughs = ['MANHATTAN', 'BROOKLYN', 'QUEENS', 'BRONX', 'STATEN ISLAND']\n",
    "    \n",
    "    for borough in boroughs_311:\n",
    "        if borough and borough.upper() in nyc_boroughs:\n",
    "            # Count records for this borough\n",
    "            count_311 = df_311[df_311['Borough'] == borough].shape[0]\n",
    "            print(f\"  {borough}: {count_311:,} 311 requests\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# 3. Community Board analysis as possible key\n",
    "print(\"🏘️ 3. COMMUNITY BOARD ANALYSIS:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if 'Community Board' in df_311.columns:\n",
    "    cb_311 = df_311['Community Board'].dropna().unique()\n",
    "    print(f\"Community Board in 311 ({len(cb_311)} unique):\")\n",
    "    print(f\"Examples: {sorted(cb_311)[:10]}\")\n",
    "    \n",
    "    # Check if there are similar fields in Capital\n",
    "    cb_like_fields = [col for col in df_capital.columns if 'board' in col.lower() or 'community' in col.lower()]\n",
    "    print(f\"\\nSimilar fields in Capital: {cb_like_fields}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# 4. Coordinates analysis\n",
    "print(\"📍 4. COORDINATES ANALYSIS:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if 'Latitude' in df_311.columns and 'Longitude' in df_311.columns:\n",
    "    lat_count = df_311['Latitude'].notna().sum()\n",
    "    lon_count = df_311['Longitude'].notna().sum()\n",
    "    print(f\"311 dataset: {lat_count:,} records with latitude coordinates, {lon_count:,} with longitude\")\n",
    "    \n",
    "    print(f\"311 coordinate ranges:\")\n",
    "    print(f\"  Latitude: {df_311['Latitude'].min():.4f} - {df_311['Latitude'].max():.4f}\")\n",
    "    print(f\"  Longitude: {df_311['Longitude'].min():.4f} - {df_311['Longitude'].max():.4f}\")\n",
    "\n",
    "# Check if there are coordinates in Capital\n",
    "coord_fields_capital = [col for col in df_capital.columns if any(word in col.lower() for word in ['lat', 'lon', 'coord'])]\n",
    "print(f\"\\nCoordinate fields in Capital: {coord_fields_capital if coord_fields_capital else 'No explicit coordinate fields'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# 5. Summary of possible join strategies\n",
    "print(\"💡 5. POSSIBLE JOIN STRATEGIES:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "strategies = [\n",
    "    {\n",
    "        'name': 'Geographic join via Borough/District',\n",
    "        'feasible': bool(boroughs_311 and districts_capital),\n",
    "        'description': 'Mapping Borough (311) -> Geographic District (Capital)',\n",
    "        'challenge': 'Need additional mapping between Borough and District numbers'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Spatial join via coordinates',\n",
    "        'feasible': lat_count > 0 and len(coord_fields_capital) == 0,\n",
    "        'description': 'Using 311 coordinates to determine proximity to Capital projects',\n",
    "        'challenge': 'Capital projects lack coordinates - need geocoding'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Temporal-geographic join',\n",
    "        'feasible': True,\n",
    "        'description': 'Combining temporal overlap + geographic proximity',\n",
    "        'challenge': 'Need additional geographic reference'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Join via external sources',\n",
    "        'feasible': True,\n",
    "        'description': 'Using additional NYC geographic reference data',\n",
    "        'challenge': 'Need external data for mapping'\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, strategy in enumerate(strategies, 1):\n",
    "    status = \"✅ Possible\" if strategy['feasible'] else \"❌ Difficult\"\n",
    "    print(f\"{i}. {strategy['name']} - {status}\")\n",
    "    print(f\"   Description: {strategy['description']}\")\n",
    "    print(f\"   Challenge: {strategy['challenge']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 5. Conclusions and Recommendations\n",
    "\n",
    "Based on the conducted analysis, here are the summary conclusions regarding data joining possibilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final conclusions and recommendations\n",
    "print(\"=\" * 80)\n",
    "print(\"📋 FINAL CONCLUSIONS AND RECOMMENDATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n✅ 1. TEMPORAL DIMENSION (TIME OVERLAP)\")\n",
    "print(\"-\" * 50)\n",
    "print(\"RESULT: Significant temporal overlap exists!\")\n",
    "print(f\"• Overlap period: 18 days (2019-11-12 to 2019-12-01)\")\n",
    "print(f\"• 311 requests in period: 100,000 records\")\n",
    "print(f\"• Capital projects: 74 projects started in this period\")\n",
    "print(\"• Overall Capital Projects period: 2003-2023 (covers all possible 311 periods)\")\n",
    "\n",
    "print(\"\\n✅ 2. GEOGRAPHIC DIMENSION\")\n",
    "print(\"-\" * 50)\n",
    "print(\"RESULT: Geographic joining possibilities exist!\")\n",
    "print(\"• 311 dataset has Borough (5 NYC boroughs) + coordinates (96,696 records)\")\n",
    "print(\"• Capital Projects has Geographic District (33 districts)\")\n",
    "print(\"• Community Board in 311 (76 unique districts)\")\n",
    "print(\"• Coordinates exist only in 311, Capital Projects lacks them\")\n",
    "\n",
    "print(\"\\n💡 3. RECOMMENDED JOIN STRATEGIES\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "strategies = [\n",
    "    {\n",
    "        \"priority\": \"High\",\n",
    "        \"name\": \"Borough → District Mapping\",\n",
    "        \"description\": \"Create mapping between Borough (311) and Geographic District (Capital)\",\n",
    "        \"implementation\": \"Use NYC School Districts or Community Districts reference\",\n",
    "        \"pros\": \"Direct relationship, high accuracy\",\n",
    "        \"cons\": \"Requires additional reference data\"\n",
    "    },\n",
    "    {\n",
    "        \"priority\": \"Medium\", \n",
    "        \"name\": \"Temporal-geographic join\",\n",
    "        \"description\": \"Combine temporal overlap + geographic proximity\",\n",
    "        \"implementation\": \"Filter by time + group by Borough/District\",\n",
    "        \"pros\": \"Enables analysis of construction impact on requests\",\n",
    "        \"cons\": \"More complex logic, requires validation\"\n",
    "    },\n",
    "    {\n",
    "        \"priority\": \"Low\",\n",
    "        \"name\": \"Geocoding + spatial join\", \n",
    "        \"description\": \"Add coordinates to Capital Projects via geocoding\",\n",
    "        \"implementation\": \"Geocode project addresses, use search radius\",\n",
    "        \"pros\": \"Most accurate spatial joining\",\n",
    "        \"cons\": \"Requires geocoding, computationally intensive\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, strategy in enumerate(strategies, 1):\n",
    "    print(f\"\\n{i}. {strategy['name']} (Priority: {strategy['priority']})\")\n",
    "    print(f\"   📝 Description: {strategy['description']}\")\n",
    "    print(f\"   🔨 Implementation: {strategy['implementation']}\")\n",
    "    print(f\"   ✅ Advantages: {strategy['pros']}\")\n",
    "    print(f\"   ⚠️  Disadvantages: {strategy['cons']}\")\n",
    "\n",
    "print(f\"\\n🎯 4. BEST APPROACH FOR ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Recommended combined strategy:\")\n",
    "print(\"1️⃣ Temporal filtering: select overlap period\")\n",
    "print(\"2️⃣ Geographic grouping: Borough (311) + additional mapping to District\")\n",
    "print(\"3️⃣ Correlation analysis: requests before/during/after projects\")\n",
    "print(\"4️⃣ Visualization: maps with overlaid zones and time series\")\n",
    "\n",
    "print(f\"\\n📊 5. EXPECTED ANALYSIS RESULTS\")\n",
    "print(\"-\" * 50)\n",
    "research_questions = [\n",
    "    \"Do 311 requests increase during active construction projects?\",\n",
    "    \"What types of requests are most commonly related to construction work?\", \n",
    "    \"In which districts does construction most impact citizen requests?\",\n",
    "    \"How long does the impact of construction projects on request volume last?\",\n",
    "    \"Are there seasonal or weekly patterns in the relationship between projects and requests?\"\n",
    "]\n",
    "\n",
    "for i, question in enumerate(research_questions, 1):\n",
    "    print(f\"{i}. {question}\")\n",
    "\n",
    "print(f\"\\n📋 6. NEXT STEPS\")\n",
    "print(\"-\" * 50)\n",
    "next_steps = [\n",
    "    \"Load complete 311 dataset (not just 100K records)\",\n",
    "    \"Find or create Borough → Geographic District mapping\",\n",
    "    \"Implement temporal-geographic join\", \n",
    "    \"Conduct exploratory analysis of joined data\",\n",
    "    \"Create visualizations to test hypotheses\",\n",
    "    \"Statistically verify correlations between projects and requests\"\n",
    "]\n",
    "\n",
    "for i, step in enumerate(next_steps, 1):\n",
    "    print(f\"{i}. {step}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python daw .venv",
   "language": "python",
   "name": "myproj-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
