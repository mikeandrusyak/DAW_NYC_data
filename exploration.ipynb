{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Exploration of the Dataset\n",
    "This notebook performs the **first stage of data wrangling** for the NYC 311 Service Requests dataset.  \n",
    "The goal is to **collect, clean, and prepare representative samples** from the raw data to support further **exploration and analysis** in later steps.\n",
    "\n",
    "We connect to the official [NYC Open Data API](https://data.cityofnewyork.us/), download data in manageable chunks per quarter,  \n",
    "and generate proportional samples across boroughs — ensuring that the dataset remains statistically representative while keeping it computationally efficient.\n",
    "\n",
    "This notebook lays the groundwork for:\n",
    "- **Merging and unifying** large raw datasets  \n",
    "- **Sampling** data per borough and time period  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import logging\n",
    "import re\n",
    "from sodapy import Socrata\n",
    "from libs.fetcher import fetch_count_of_grouping, fetch_all_samples_from_plan\n",
    "from libs.utils import generate_quarters\n",
    "from libs.calculator import calc_sample_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## ⚙️ Constants and Configuration\n",
    "\n",
    "The following section defines key constants used throughout this notebook:\n",
    "- **BASE_URL:** the NYC Open Data API endpoint for 311 Service Requests  \n",
    "- **DEFAULT_SINCE / DEFAULT_UNTIL:** default year range for sampling  \n",
    "- **TARGET_SAMPLE:** target number of records for each quarterly sample  \n",
    "- **DEFAULT_DB_PATH / DEFAULT_TABLE:** optional configuration for local DuckDB storage  \n",
    "- **MAX_RETRIES / TIMEOUT / BASE_DELAY:** network parameters for reliable API requests  \n",
    "- **SELECT_COLUMNS:** the list of columns (fields) to retrieve from the API  \n",
    "- **data_sets:** any additional CSV datasets used for contextual enrichment (e.g., housing, demographic, or rent data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "BASE_URL = \"https://data.cityofnewyork.us/resource/erm2-nwe9.csv\"\n",
    "DEFAULT_SINCE = 2024\n",
    "DEFAULT_UNTIL = 2025\n",
    "TARGET_SAMPLE = 10_000\n",
    "DEFAULT_DB_PATH = \"./mydb.duckdb\"\n",
    "DEFAULT_TABLE = \"nyc311_2024_2025\"\n",
    "MAX_RETRIES = 5\n",
    "TIMEOUT = 60  # seconds\n",
    "BASE_DELAY = 2.0  # seconds\n",
    "\n",
    "SELECT_COLUMNS = [\n",
    "    \"unique_key\", \"created_date\", \"closed_date\", \"agency\", \"agency_name\", \n",
    "    \"complaint_type\", \"descriptor\", \"location_type\", \"incident_zip\", \n",
    "    \"incident_address\", \"street_name\", \"cross_street_1\", \"cross_street_2\",\n",
    "    \"intersection_street_1\", \"intersection_street_2\", \"address_type\", \"city\", \n",
    "    \"landmark\", \"facility_type\", \"status\", \"due_date\", \"resolution_description\", \n",
    "    \"resolution_action_updated_date\", \"community_board\", \"bbl\", \"borough\", \n",
    "    \"x_coordinate_state_plane\", \"y_coordinate_state_plane\", \"open_data_channel_type\",\n",
    "    \"park_facility_name\", \"park_borough\", \"vehicle_type\", \"taxi_company_borough\", \n",
    "    \"taxi_pick_up_location\", \"bridge_highway_name\", \"bridge_highway_direction\", \n",
    "    \"road_ramp\", \"bridge_highway_segment\", \"latitude\", \"longitude\", \"location\"\n",
    "]\n",
    "\n",
    "data_sets = [\n",
    "     \"data/medianAskingRent_All.csv\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "##  1. Workflow Overview\n",
    "\n",
    "1. **Generate Quarterly Ranges:**  \n",
    "   For each year between the selected start and end years, we create `(start, end)` date pairs.  \n",
    "   Example: `2024-01-01T00:00:00` → `2024-03-31T23:59:59` (Q1 2024)\n",
    "\n",
    "2. **Fetch Borough Counts:**  \n",
    "   Using the Socrata API, we retrieve the total number of service requests per `borough` within each quarter.  \n",
    "   → Output: a list or DataFrame with columns  \n",
    "   `['borough', 'total']`\n",
    "\n",
    "3. **Compute Sampling Plan:**  \n",
    "   Based on each borough’s proportion of total records, we calculate how many samples to take per borough:  \n",
    "   $$\n",
    "   n_i = N_\\text{sample} \\times \\frac{\\text{total}_i}{\\text{total}_\\text{overall}}\n",
    "   $$\n",
    "   The result is a sampling plan with one `sample_size` value per borough.\n",
    "\n",
    "4. **Fetch Random Samples:**  \n",
    "   For each borough, we randomly pull `sample_size` records from the corresponding quarter using the Socrata API.  \n",
    "   - Data is retrieved via the `.csv` endpoint (faster than JSON).  \n",
    "   - Optionally: a random `$offset` and local random sampling (`.sample()` in Pandas) ensure randomness.\n",
    "\n",
    "5. **Combine All Quarters:**  \n",
    "   The sampled data from all boroughs and quarters are concatenated into a single combined DataFrame using  \n",
    "   `pd.concat(all_quarters, ignore_index=True)`.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ 2. Key Functions\n",
    "\n",
    "| Function | Description |\n",
    "|-----------|--------------|\n",
    "| `generate_quarters(start_year, end_year)` | Generates quarterly date ranges |\n",
    "| `fetch_count_of_grouping(BASE_URL, group_by, start, end)` | Retrieves counts per group (e.g., borough) |\n",
    "| `calc_sample_size(count_result)` | Computes proportional sample sizes |\n",
    "| `fetch_random_sample(...)` | Fetches a random subset for one borough and quarter |\n",
    "| `fetch_all_samples_from_plan(...)` | Iterates over boroughs and collects their samples |\n",
    "| `fetch_all_quarters(...)` *(optional)* | Runs the entire pipeline across all quarters |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch sample of datasets and parse to Data Frame \n",
    "\n",
    "# 1. generate the time ranges:\n",
    "quarters = generate_quarters(DEFAULT_SINCE, DEFAULT_UNTIL)\n",
    "data_frames = []\n",
    "\n",
    "for start, end in quarters:\n",
    "    logging.info(f\"Quarter from {start} to {end}\")\n",
    "    # 2. fetch count of each borough -> (This can be changed according to the needs)in the time range\n",
    "    count_result = fetch_count_of_grouping(BASE_URL, \"borough\", start, end)\n",
    "    # 3. calculate sample sizes\n",
    "    df_plan = calc_sample_size(count_result, TARGET_SAMPLE)\n",
    "    # 4. fetch samples according to the plan\n",
    "    df_311_calls = fetch_all_samples_from_plan(\n",
    "                    BASE_URL=BASE_URL,\n",
    "                    selectors=SELECT_COLUMNS,\n",
    "                    df_plan=df_plan,\n",
    "                    group_by=\"borough\", \n",
    "                    time_start=start,\n",
    "                    time_end=end,\n",
    "                    sleep_seconds=BASE_DELAY\n",
    "    )\n",
    "    df_311_calls[\"quarter_start\"] = start\n",
    "    df_311_calls[\"quarter_end\"] = end\n",
    "    data_frames.append(df_311_calls)\n",
    "\n",
    "# Combine all quarters into a single DataFrame\n",
    "df_all_calls = pd.concat(data_frames, ignore_index=True)\n",
    "logging.info(f\"Total records fetched: {len(df_all_calls)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lead of the CSV file and conversion in Parquet\n",
    "con = duckdb.connect(\"mydb.duckdb\")\n",
    "\n",
    "for csv_file in data_sets:\n",
    "    # Determine the filename for Parquet and name the table\n",
    "    parquet_file = csv_file.replace(\".csv\", \".parquet\")\n",
    "    table_name = csv_file.split(\"/\")[-1].replace(\".csv\", \"\").replace(\"-\", \"_\")\n",
    "    # Table name must not start with number\n",
    "    if re.match(r'^\\d', table_name):\n",
    "        table_name = \"t_\" + table_name\n",
    "\n",
    "    print(f\"Tablename: {table_name}\")\n",
    "    try:\n",
    "        con.execute(f\"\"\"\n",
    "            COPY (SELECT * FROM read_csv_auto('{csv_file}'))\n",
    "            TO '{parquet_file}' (FORMAT 'parquet');\n",
    "        \"\"\")\n",
    "        print(f\"Conversion successfull {csv_file} → {parquet_file}\")\n",
    "        print(\"--------------------------------\")\n",
    "        con.execute(f\"\"\"\n",
    "            CREATE OR REPLACE TABLE {table_name} AS\n",
    "            SELECT * FROM read_parquet('{parquet_file}');\n",
    "        \"\"\")\n",
    "        print(f\"{csv_file} Table successfully converted!\")\n",
    "        print(\"--------------------------------\")\n",
    "        print(\"--------------------------------\")\n",
    "    except Exception as e:\n",
    "        print(\"❌ Failed to convert:\", e)\n",
    "\n",
    "print(\"CSV successfully converted to parquet and Table created!\")\n",
    "\n",
    "# Generates quarters from 2021 to 2025\n",
    "quarters = generate_quarters(2024, 2025)\n",
    "\n",
    "# Fetch data from Socrata API and create table\n",
    "client = Socrata(\"data.cityofnewyork.us\",\n",
    "                  \"Upkc825Y13IyAEMlWSq4kL2dz\",\n",
    "                  username=\"roberto.fazekas.priv@gmail.com\",\n",
    "                  password=\"jugGom-kypcom-pytsu3\",\n",
    "                  timeout=120)\n",
    "\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establishing the connection to the DuckDB database\n",
    "con = duckdb.connect(\"mydb.duckdb\")\n",
    "\n",
    "df_affordable = con.execute(\"\"\"\n",
    "    SELECT *\n",
    "    FROM medianAskingRent_All\n",
    "    LIMIT 100\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "res = con.execute(\"SELECT COUNT(*) FROM calls_311\").fetchone()[0]\n",
    "print(\"Number of rows:\", res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python daw .venv",
   "language": "python",
   "name": "myproj-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
