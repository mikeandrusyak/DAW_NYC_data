{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "Final Submit DAW\n",
    "<br>\n",
    "Authors: Mychailo & Roberto\n",
    "\n",
    "#  **New York State of Mind**\n",
    "\n",
    "##  **Introduction**\n",
    "\n",
    "This notebook presents the **final report** for our project in the module  \n",
    "**Data Wrangling (DAW)**.  \n",
    "\n",
    "It is based on two primary datasets:  \n",
    "\n",
    "- [NYC 311 Service Requests](https://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9/about_data)  \n",
    "- [StreetEasy Data Dashboard](https://streeteasy.com/blog/data-dashboard/?utm_source=chatgpt.com)  \n",
    "\n",
    "---\n",
    "\n",
    "The project demonstrates a complete **data wrangling pipeline**, covering the topics of the LE's in this module.\n",
    "\n",
    "1. **Importing** – retrieving and sampling real-world open data  \n",
    "2. **Data Cleaning** – preparing data for analysis  \n",
    "3. **Tranforming** – further preparation feature engineering  \n",
    "4. **Joining** - joining the data\n",
    "5. **Data Pipelines** – bla bla\n",
    "6. **Reproducability** - bla bla\n",
    "\n",
    "---\n",
    "\n",
    "Our work follows the structure of the **commonly used data exploration framework**, \n",
    "\n",
    "---\n",
    "<img src=\"./img/data_exploring.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "\n",
    "## LE1 Importing the Data\n",
    "###  1. Workflow Overview\n",
    "\n",
    "1. **Generate month range:**  \n",
    "   For each year between the selected start and end years, we create `(start, end)` date pairs.  \n",
    "   First we create quarterly ranges and then split them into the months.\n",
    "   Example: `2024-01-01T00:00:00` → `2024-01-31T23:59:59` (M1 2024)\n",
    "\n",
    "2. **Fetch Entries per Day:**  \n",
    "   For the month we pick a amount of days, which days are random but the amount of days of the month are the same. For the amount of entries we want to fetch per day we calculate the amount as follows: \n",
    "   $$ \n",
    "      \\text{per\\_day} = \\left\\lfloor \\frac{ \\text{target} \\times \\text{per\\_day\\_mult} }{ \\lvert \\text{days} \\rvert } \\right\\rfloor\n",
    "   $$\n",
    "\n",
    "3. **Fetch Random Samples:**  \n",
    "   For each borough, we randomly pull the calculated per day amount of records from the corresponding day using the openly accessible API. \n",
    "   - Data is retrieved via the `.csv` endpoint (faster than JSON).  \n",
    "   - A random `$offset` and a random choice, acsending or descending is used in sampling ensure randomness.\n",
    "\n",
    "4. **Combine:**  \n",
    "   The sampled data from all boroughs and days in the months are concatenated into a single combined DataFrame using  \n",
    "   `pd.concat(all_quarters, ignore_index=True)`.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Key Functions\n",
    "\n",
    "| Function | Description |\n",
    "|-----------|--------------|\n",
    "| `generate_quarters(start_year, end_year)` | Generates quarterly date ranges |\n",
    "| `month_range(start, end)` | Generates the month range |\n",
    "| `fetch_month_strat_data(...)` | Fetches a random subset for one borough and day |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4481de2c",
   "metadata": {},
   "source": [
    "### Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94947617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for LE1\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from config import Settings, get_settings\n",
    "from libs.utils import generate_quarters, generate_month_ranges\n",
    "from libs.fetcher import get_dataset_stratified\n",
    "from libs_tidy.distribution import test_imported_data_distribution_light, plot_distribution\n",
    "from libs_tidy.tidying import prepare_date_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba560d5",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9df9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants \n",
    "SETTINGS: Settings = get_settings()\n",
    "\n",
    "print(\"Loaded config:\")\n",
    "print(f\"{SETTINGS.GROUP_BY}\")\n",
    "print(f\"{SETTINGS.GROUP_BY_VALUE}\")\n",
    "print(f\"{SETTINGS.PLOT_DIST}\")\n",
    "\n",
    "# TODO: Maybe put this into a env variable\n",
    "# Selection\n",
    "SELECT_COLUMNS = [\n",
    "    \"unique_key\", \"created_date\", \"closed_date\", \"agency\", \"agency_name\", \n",
    "    \"complaint_type\", \"descriptor\", \"location_type\", \"incident_zip\", \n",
    "    \"incident_address\", \"street_name\", \"cross_street_1\", \"cross_street_2\",\n",
    "    \"intersection_street_1\", \"intersection_street_2\", \"address_type\", \"city\", \n",
    "    \"landmark\", \"facility_type\", \"status\", \"due_date\", \"resolution_description\", \n",
    "    \"resolution_action_updated_date\", \"community_board\", \"bbl\", \"borough\", \n",
    "    \"x_coordinate_state_plane\", \"y_coordinate_state_plane\", \"open_data_channel_type\",\n",
    "    \"park_facility_name\", \"park_borough\", \"vehicle_type\", \"taxi_company_borough\", \n",
    "    \"taxi_pick_up_location\", \"bridge_highway_name\", \"bridge_highway_direction\", \n",
    "    \"road_ramp\", \"bridge_highway_segment\", \"latitude\", \"longitude\", \"location\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a9462b",
   "metadata": {},
   "source": [
    "### Fetch and Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch sample of datasets and parse to Data Frame \n",
    "\n",
    "# 1. generate the time ranges:\n",
    "quarters = generate_quarters(SETTINGS.DEFAULT_SINCE, SETTINGS.DEFAULT_UNTIL)\n",
    "months = generate_month_ranges(quarters)\n",
    "# 2. fetch the data \n",
    "df_all_calls = get_dataset_stratified(months, SETTINGS, SELECT_COLUMNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9002e1a",
   "metadata": {},
   "source": [
    "### Safe data to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11fea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_calls.to_csv(\"data/nyc_311_2024_2025_sample.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Testing the import of data\n",
    "\n",
    "To verify our function to fetch a representitv sample worked, it is nessacary to:\n",
    "1. verify if the distribution over time is the same? \n",
    "\n",
    "The second step in this procedure could be considered part of the tidy step in eplorational data analysis workflow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the import \n",
    "\n",
    "# TODO: Maybe think about putting this test into tidying of data, but question is, what kinda test will we put here when we only test the import in the tidying of the data. Maybe a Test here could be to test Endpoint connection? \n",
    "\n",
    "# Read the saved dataset\n",
    "df_nyc_311_2024_2025 = pd.read_csv(\"data/nyc_311_2024_2025_sample.csv\")\n",
    "df_nyc_311_2024_2025 = prepare_date_time(df_nyc_311_2024_2025)\n",
    "\n",
    "\n",
    "for month, group in df_nyc_311_2024_2025.groupby('create_month'):\n",
    "    # Plot if Plot wanted \n",
    "    if SETTINGS.PLOT_DIST:\n",
    "        plot_distribution(group, month)\n",
    "\n",
    "\n",
    "# Call the test function    \n",
    "test_imported_data_distribution_light(df_nyc_311_2024_2025, max_cv=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cd5f41",
   "metadata": {},
   "source": [
    "## LE2 Tidying the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43b6b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the Tidying we look at the shape and the basic information for the datasets\n",
    "\n",
    "df_nyc_311 = pd.read_csv('data/nyc_311_2024_2025_sample.csv', index_col=\"unique_key\")\n",
    "df_median_rent = pd.read_csv('data/medianAskingRent_All.csv')\n",
    "\n",
    "print(f\"NYC 311 data shape: {df_nyc_311.shape}\")\n",
    "print(f\"Median rent data shape: {df_median_rent.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python daw .venv",
   "language": "python",
   "name": "myproj-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
