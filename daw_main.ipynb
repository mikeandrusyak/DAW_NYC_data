{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76d004c0",
   "metadata": {},
   "source": [
    "## Library Import\n",
    "\n",
    "First, we import the necessary components from our simplified libs_daw library. The main component is the `NYCDataPipeline` class, which encapsulates all the data processing functionality in a single, easy-to-use interface.\n",
    "\n",
    "The library is designed to be lightweight and focused specifically on Method 2 (step-by-step pipeline usage), making it ideal for educational purposes and detailed data analysis workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea29d9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import simplified pipeline\n",
    "from libs_daw import NYCDataPipeline\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Simplified libs_daw library successfully imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322bcc51",
   "metadata": {},
   "source": [
    "## Step-by-Step Pipeline Usage\n",
    "\n",
    "We use 4 main data processing steps, each building upon the previous one:\n",
    "\n",
    "1. **Data Loading**: Import raw data from various sources (CSV files, JSON mappings)\n",
    "2. **Data Cleaning**: Remove inconsistencies, handle missing values, standardize formats\n",
    "3. **Data Transformation**: Create new features, apply geographic mappings, prepare for analysis\n",
    "4. **Aggregation and Integration**: Combine datasets and create the final analytical dataset\n",
    "\n",
    "This modular approach allows for:\n",
    "- **Transparency**: Each step can be inspected and validated independently\n",
    "- **Flexibility**: Individual steps can be modified without affecting others\n",
    "- **Debugging**: Easy identification of issues at specific processing stages\n",
    "- **Learning**: Clear understanding of the data transformation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3303a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline initialization\n",
    "pipeline = NYCDataPipeline()\n",
    "\n",
    "print(\"Pipeline initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9674396",
   "metadata": {},
   "source": [
    "### Step 1: Data Loading\n",
    "\n",
    "**Purpose**: Import all necessary data sources into memory for processing.\n",
    "\n",
    "**What happens in this step**:\n",
    "- Loads NYC 311 service requests data from CSV file\n",
    "- Imports median rent data by neighborhood and time period\n",
    "- Reads UHF (United Hospital Fund) geographic mapping data from JSON\n",
    "- Loads manual mapping corrections for data quality improvements\n",
    "\n",
    "**Data Sources**:\n",
    "- `nyc_311_2024_2025_sample.csv`: Sample of citizen service requests\n",
    "- `medianAskingRent_All.csv`: Rental market data by area\n",
    "- `nyc_uhf_zipcodes.json`: Geographic boundary definitions\n",
    "- `manual_map.json`: Manual corrections for geographic mappings\n",
    "\n",
    "**Expected Outcome**: Four separate datasets ready for cleaning and processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a6cb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data from csv\n",
    "df_nyc_311, df_median_rent, uhf_data, manual_map = pipeline.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fae52d1",
   "metadata": {},
   "source": [
    "### Step 2: Data Cleaning\n",
    "\n",
    "**Purpose**: Standardize and clean the raw data to ensure quality and consistency.\n",
    "\n",
    "**What happens in this step**:\n",
    "\n",
    "**For NYC 311 Data**:\n",
    "- Removes records with missing critical information (location, complaint type)\n",
    "- Standardizes date formats and extracts temporal features\n",
    "- Cleans and normalizes text fields (complaint descriptions, addresses)\n",
    "- Validates and corrects geographic coordinates\n",
    "- Removes duplicate entries and obvious data entry errors\n",
    "\n",
    "**For Median Rent Data**:\n",
    "- Handles missing rent values using appropriate imputation methods\n",
    "- Standardizes neighborhood names for consistent mapping\n",
    "- Validates date ranges and removes outliers\n",
    "- Ensures proper numeric formatting for rent amounts\n",
    "\n",
    "**Expected Outcome**: Clean, standardized datasets ready for feature engineering and transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d90cab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean both datasets\n",
    "df_nyc_311_cleaned, df_median_rent_cleaned = pipeline.clean_data(\n",
    "    df_nyc_311, df_median_rent\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddae8089",
   "metadata": {},
   "source": [
    "### Step 3: Data Transformation\n",
    "\n",
    "**Purpose**: Create new features and apply geographic mappings to prepare data for analysis.\n",
    "\n",
    "**What happens in this step**:\n",
    "\n",
    "**Geographic Transformation**:\n",
    "- Maps ZIP codes to UHF neighborhoods using the geographic data\n",
    "- Applies manual mapping corrections for edge cases\n",
    "- Creates standardized neighborhood identifiers\n",
    "- Validates geographic consistency across datasets\n",
    "\n",
    "**Feature Engineering**:\n",
    "- Extracts temporal features (year, month, season) from dates\n",
    "- Categorizes complaint types into broader analytical groups\n",
    "- Creates derived metrics (complaints per capita, rent change rates)\n",
    "- Generates location-based features for spatial analysis\n",
    "\n",
    "**Data Harmonization**:\n",
    "- Ensures both datasets use consistent geographic boundaries\n",
    "- Aligns temporal periods between 311 and rent data\n",
    "- Creates join keys for later integration\n",
    "\n",
    "**Expected Outcome**: Transformed datasets with rich features and consistent geographic mappings, ready for integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d50c865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data and create new features\n",
    "df_nyc_311_transformed, df_median_rent_transformed = pipeline.transform_data(\n",
    "    df_nyc_311_cleaned, df_median_rent_cleaned, uhf_data, manual_map\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd4801e",
   "metadata": {},
   "source": [
    "### Step 4: Aggregation and Integration\n",
    "\n",
    "**Purpose**: Combine the transformed datasets into a single, analysis-ready dataset.\n",
    "\n",
    "**What happens in this step**:\n",
    "\n",
    "**Data Aggregation**:\n",
    "- Aggregates 311 complaints by neighborhood and time period\n",
    "- Calculates complaint frequency metrics and trends\n",
    "- Summarizes rent data by geographic area and temporal windows\n",
    "- Computes statistical measures (means, medians, trends) for both datasets\n",
    "\n",
    "**Dataset Integration**:\n",
    "- Joins 311 and rent data using standardized geographic keys\n",
    "- Handles temporal alignment between different data collection periods\n",
    "- Resolves any remaining data inconsistencies\n",
    "- Creates composite metrics combining both data sources\n",
    "\n",
    "**Quality Assurance**:\n",
    "- Validates the integrated dataset for completeness\n",
    "- Checks for logical consistency across combined features\n",
    "- Ensures proper data types and value ranges\n",
    "- Documents any limitations or caveats in the final dataset\n",
    "\n",
    "**Expected Outcome**: A single, comprehensive dataset ready for analysis, visualization, and modeling, containing both service request patterns and housing market information by NYC neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7110bde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine and create final dataset\n",
    "final_dataset = pipeline.aggregate_and_integrate(\n",
    "    df_nyc_311_transformed, df_median_rent_transformed\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal dataset created: {final_dataset.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04f3089",
   "metadata": {},
   "source": [
    "## Results Overview\n",
    "\n",
    "**Purpose**: Examine the final integrated dataset to understand its structure and quality.\n",
    "\n",
    "This section provides a comprehensive overview of the processed data, including:\n",
    "\n",
    "**Dataset Structure Analysis**:\n",
    "- Dimensions (rows and columns) of the final dataset\n",
    "- Column names and their meanings\n",
    "- Data types and value ranges\n",
    "\n",
    "**Data Quality Assessment**:\n",
    "- Completeness rates for each variable\n",
    "- Distribution of key categorical variables\n",
    "- Temporal and geographic coverage\n",
    "\n",
    "**Sample Data Review**:\n",
    "- Representative sample of the integrated records\n",
    "- Examples of how 311 complaints and rent data are combined\n",
    "- Validation of the integration logic\n",
    "\n",
    "This overview helps verify that the data processing pipeline has successfully created a usable dataset for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d28c180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review final dataset structure\n",
    "print(\"=== FINAL DATASET STRUCTURE ===\")\n",
    "print(f\"Size: {final_dataset.shape}\")\n",
    "print(f\"Columns: {list(final_dataset.columns)}\")\n",
    "\n",
    "print(\"\\n=== DATA SAMPLE ===\")\n",
    "display(final_dataset.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a25e1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"=== BASIC STATISTICS ===\")\n",
    "print(f\"Total records: {len(final_dataset):,}\")\n",
    "print(f\"Unique neighborhoods: {final_dataset['neighborhood'].nunique()}\")\n",
    "print(f\"Unique complaint types: {final_dataset['complaint_type'].nunique()}\")\n",
    "print(f\"Year range: {final_dataset['year'].min()}-{final_dataset['year'].max()}\")\n",
    "\n",
    "print(\"\\n=== DATA COMPLETENESS ===\")\n",
    "completeness = (final_dataset.notna().sum() / len(final_dataset) * 100).round(1)\n",
    "for col, pct in completeness.items():\n",
    "    print(f\"{col}: {pct}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caf0d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset.to_csv('data/data_snapshot_for_gdv.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
