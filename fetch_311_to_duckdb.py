#!/usr/bin/env python3
"""
Production-ready script to fetch NYC 311 Service Requests data for 2024-2025
and load into DuckDB with efficient upsert capabilities.

Author: Generated by GitHub Copilot
Date: 2025-10-02
"""

import argparse
import logging
import os
import sys
import time
from datetime import datetime
from io import BytesIO
from typing import Dict, List, Optional
from urllib.parse import urlencode, quote_plus

import pandas as pd
import requests
import duckdb

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# Constants
BASE_URL = "https://data.cityofnewyork.us/resource/erm2-nwe9.csv"
DEFAULT_SINCE = "2024-01-01T00:00:00"
DEFAULT_UNTIL = "2025-12-31T23:59:59"
DEFAULT_PAGE_SIZE = 50000
DEFAULT_DB_PATH = "./mydb.duckdb"
DEFAULT_TABLE = "nyc311_2024_2025"

# Required columns in order
SELECT_COLUMNS = [
    "unique_key", "created_date", "closed_date", "agency", "agency_name", 
    "complaint_type", "descriptor", "location_type", "incident_zip", 
    "incident_address", "street_name", "cross_street_1", "cross_street_2",
    "intersection_street_1", "intersection_street_2", "address_type", "city", 
    "landmark", "facility_type", "status", "due_date", "resolution_description", 
    "resolution_action_updated_date", "community_board", "bbl", "borough", 
    "x_coordinate_state_plane", "y_coordinate_state_plane", "open_data_channel_type",
    "park_facility_name", "park_borough", "vehicle_type", "taxi_company_borough", 
    "taxi_pick_up_location", "bridge_highway_name", "bridge_highway_direction", 
    "road_ramp", "bridge_highway_segment", "latitude", "longitude", "location"
]

def build_soql_url(select_cols: List[str], since: str, until: str, limit: int, offset: int) -> str:
    """Build SoQL CSV URL with proper encoding."""
    select_clause = ",".join(select_cols)
    where_clause = f"created_date BETWEEN '{since}' AND '{until}'"
    
    params = {
        "$select": select_clause,
        "$where": where_clause,
        "$order": "created_date DESC",
        "$limit": str(limit),
        "$offset": str(offset)
    }
    
    # Manual URL encoding for SoQL parameters
    encoded_params = "&".join([f"{k}={quote_plus(v)}" for k, v in params.items()])
    return f"{BASE_URL}?{encoded_params}"

def http_get_csv(url: str, headers: Optional[Dict[str, str]] = None) -> bytes:
    """HTTP GET with robust retry logic and backoff."""
    if headers is None:
        headers = {}
    
    max_retries = 7  # Increased from 5 to 7
    base_delay = 2.0  # Increased from 1.0 to 2.0
    timeout = 60  # Increased from 30 to 60 seconds
    
    for attempt in range(max_retries):
        try:
            response = requests.get(url, headers=headers, timeout=timeout)
            
            if response.status_code == 200:
                return response.content
            elif response.status_code == 429:
                # Rate limited - longer backoff
                delay = base_delay * (3 ** attempt)
                logger.warning(f"Rate limited (429). Waiting {delay:.1f}s before retry {attempt + 1}/{max_retries}")
                time.sleep(delay)
            elif response.status_code >= 500:
                # Server error - exponential backoff
                delay = base_delay * (2 ** attempt)
                logger.warning(f"Server error {response.status_code}. Waiting {delay:.1f}s before retry {attempt + 1}/{max_retries}")
                time.sleep(delay)
            else:
                # Client error - don't retry
                response.raise_for_status()
                
        except (requests.exceptions.Timeout, requests.exceptions.ConnectionError) as e:
            delay = base_delay * (2 ** attempt)
            logger.warning(f"Network error: {e}. Waiting {delay:.1f}s before retry {attempt + 1}/{max_retries}")
            time.sleep(delay)
    
    raise Exception(f"Failed to fetch data after {max_retries} attempts")

def parse_csv_to_df(csv_bytes: bytes) -> pd.DataFrame:
    """Parse CSV bytes to DataFrame with proper dtypes."""
    df = pd.read_csv(BytesIO(csv_bytes), dtype=str, na_values=['', 'N/A', 'NULL'])
    
    if df.empty:
        return df
    
    # Parse datetime columns to UTC
    datetime_cols = ['created_date', 'closed_date', 'due_date', 'resolution_action_updated_date']
    for col in datetime_cols:
        if col in df.columns:
            df[col] = pd.to_datetime(df[col], errors='coerce', utc=True)
    
    # Convert coordinate columns to numeric
    numeric_cols = ['latitude', 'longitude', 'x_coordinate_state_plane', 'y_coordinate_state_plane']
    for col in numeric_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
    
    # Ensure text columns for IDs and zip codes
    text_cols = ['unique_key', 'bbl', 'incident_zip', 'community_board']
    for col in text_cols:
        if col in df.columns:
            df[col] = df[col].astype(str)
            df[col] = df[col].replace('nan', None)
    
    return df

def connect_duckdb(db_path: str) -> duckdb.DuckDBPyConnection:
    """Connect to DuckDB with performance optimizations."""
    conn = duckdb.connect(db_path)
    
    # Performance optimizations
    conn.execute("PRAGMA memory_limit='2GB'")
    conn.execute("PRAGMA threads=4")
    conn.execute("PRAGMA enable_progress_bar=false")
    
    return conn

def ensure_table(conn: duckdb.DuckDBPyConnection, table_name: str) -> None:
    """Create table if not exists with explicit schema."""
    create_sql = f"""
    CREATE TABLE IF NOT EXISTS {table_name} (
        unique_key TEXT PRIMARY KEY,
        created_date TIMESTAMP,
        closed_date TIMESTAMP,
        agency TEXT,
        agency_name TEXT,
        complaint_type TEXT,
        descriptor TEXT,
        location_type TEXT,
        incident_zip TEXT,
        incident_address TEXT,
        street_name TEXT,
        cross_street_1 TEXT,
        cross_street_2 TEXT,
        intersection_street_1 TEXT,
        intersection_street_2 TEXT,
        address_type TEXT,
        city TEXT,
        landmark TEXT,
        facility_type TEXT,
        status TEXT,
        due_date TIMESTAMP,
        resolution_description TEXT,
        resolution_action_updated_date TIMESTAMP,
        community_board TEXT,
        bbl TEXT,
        borough TEXT,
        x_coordinate_state_plane DOUBLE,
        y_coordinate_state_plane DOUBLE,
        open_data_channel_type TEXT,
        park_facility_name TEXT,
        park_borough TEXT,
        vehicle_type TEXT,
        taxi_company_borough TEXT,
        taxi_pick_up_location TEXT,
        bridge_highway_name TEXT,
        bridge_highway_direction TEXT,
        road_ramp TEXT,
        bridge_highway_segment TEXT,
        latitude DOUBLE,
        longitude DOUBLE,
        location TEXT
    )
    """
    conn.execute(create_sql)
    logger.info(f"Table {table_name} ready")

def upsert_batch(conn: duckdb.DuckDBPyConnection, df: pd.DataFrame, table_name: str) -> None:
    """Upsert batch using MERGE INTO with staging table."""
    if df.empty:
        return
    
    staging_table = f"{table_name}_staging"
    
    try:
        conn.begin()
        
        # Drop staging table if exists
        conn.execute(f"DROP TABLE IF EXISTS {staging_table}")
        
        # Create staging table with same schema
        conn.execute(f"CREATE TEMP TABLE {staging_table} AS SELECT * FROM {table_name} WHERE 1=0")
        
        # Insert data into staging table
        conn.register('df_temp', df)
        conn.execute(f"INSERT INTO {staging_table} SELECT * FROM df_temp")
        
        # Perform MERGE (upsert)
        merge_sql = f"""
        MERGE INTO {table_name} AS target
        USING {staging_table} AS source
        ON target.unique_key = source.unique_key
        WHEN MATCHED THEN UPDATE SET
            created_date = source.created_date,
            closed_date = source.closed_date,
            agency = source.agency,
            agency_name = source.agency_name,
            complaint_type = source.complaint_type,
            descriptor = source.descriptor,
            location_type = source.location_type,
            incident_zip = source.incident_zip,
            incident_address = source.incident_address,
            street_name = source.street_name,
            cross_street_1 = source.cross_street_1,
            cross_street_2 = source.cross_street_2,
            intersection_street_1 = source.intersection_street_1,
            intersection_street_2 = source.intersection_street_2,
            address_type = source.address_type,
            city = source.city,
            landmark = source.landmark,
            facility_type = source.facility_type,
            status = source.status,
            due_date = source.due_date,
            resolution_description = source.resolution_description,
            resolution_action_updated_date = source.resolution_action_updated_date,
            community_board = source.community_board,
            bbl = source.bbl,
            borough = source.borough,
            x_coordinate_state_plane = source.x_coordinate_state_plane,
            y_coordinate_state_plane = source.y_coordinate_state_plane,
            open_data_channel_type = source.open_data_channel_type,
            park_facility_name = source.park_facility_name,
            park_borough = source.park_borough,
            vehicle_type = source.vehicle_type,
            taxi_company_borough = source.taxi_company_borough,
            taxi_pick_up_location = source.taxi_pick_up_location,
            bridge_highway_name = source.bridge_highway_name,
            bridge_highway_direction = source.bridge_highway_direction,
            road_ramp = source.road_ramp,
            bridge_highway_segment = source.bridge_highway_segment,
            latitude = source.latitude,
            longitude = source.longitude,
            location = source.location
        WHEN NOT MATCHED THEN INSERT VALUES (
            source.unique_key, source.created_date, source.closed_date, source.agency,
            source.agency_name, source.complaint_type, source.descriptor, source.location_type,
            source.incident_zip, source.incident_address, source.street_name, source.cross_street_1,
            source.cross_street_2, source.intersection_street_1, source.intersection_street_2,
            source.address_type, source.city, source.landmark, source.facility_type,
            source.status, source.due_date, source.resolution_description,
            source.resolution_action_updated_date, source.community_board, source.bbl,
            source.borough, source.x_coordinate_state_plane, source.y_coordinate_state_plane,
            source.open_data_channel_type, source.park_facility_name, source.park_borough,
            source.vehicle_type, source.taxi_company_borough, source.taxi_pick_up_location,
            source.bridge_highway_name, source.bridge_highway_direction, source.road_ramp,
            source.bridge_highway_segment, source.latitude, source.longitude, source.location
        )
        """
        
        conn.execute(merge_sql)
        conn.commit()
        
    except Exception as e:
        conn.rollback()
        raise e
    finally:
        # Clean up
        conn.execute(f"DROP TABLE IF EXISTS {staging_table}")

def create_indexes(conn: duckdb.DuckDBPyConnection, table_name: str) -> None:
    """Create helpful indexes."""
    indexes = [
        f"CREATE INDEX IF NOT EXISTS idx_{table_name}_created_date ON {table_name}(created_date)",
        f"CREATE INDEX IF NOT EXISTS idx_{table_name}_borough ON {table_name}(borough)"
    ]
    
    for idx_sql in indexes:
        conn.execute(idx_sql)
    
    logger.info("Indexes created")

def print_summary(conn: duckdb.DuckDBPyConnection, table_name: str) -> None:
    """Print summary statistics."""
    # Total rows
    total_rows = conn.execute(f"SELECT COUNT(*) FROM {table_name}").fetchone()[0]
    
    # Date range
    date_range = conn.execute(f"""
        SELECT MIN(created_date) as min_date, MAX(created_date) as max_date 
        FROM {table_name}
    """).fetchone()
    
    # Sample rows
    sample_rows = conn.execute(f"""
        SELECT unique_key, created_date, complaint_type, borough, status 
        FROM {table_name} 
        ORDER BY created_date DESC 
        LIMIT 5
    """).fetchall()
    
    print("\n" + "="*80)
    print("LOAD SUMMARY")
    print("="*80)
    print(f"Total rows in table: {total_rows:,}")
    print(f"Date range: {date_range[0]} to {date_range[1]}")
    print("\nSample rows:")
    print("-" * 80)
    print(f"{'Unique Key':<15} {'Created Date':<20} {'Complaint Type':<25} {'Borough':<12} {'Status'}")
    print("-" * 80)
    
    for row in sample_rows:
        unique_key = str(row[0])[:14] if row[0] else "N/A"
        created_date = str(row[1])[:19] if row[1] else "N/A"
        complaint_type = str(row[2])[:24] if row[2] else "N/A"
        borough = str(row[3])[:11] if row[3] else "N/A"
        status = str(row[4]) if row[4] else "N/A"
        
        print(f"{unique_key:<15} {created_date:<20} {complaint_type:<25} {borough:<12} {status}")

def main():
    """Main execution function."""
    parser = argparse.ArgumentParser(description="Fetch NYC 311 data to DuckDB")
    parser.add_argument("--since", default=DEFAULT_SINCE, 
                       help=f"Start date (default: {DEFAULT_SINCE})")
    parser.add_argument("--until", default=DEFAULT_UNTIL,
                       help=f"End date (default: {DEFAULT_UNTIL})")
    parser.add_argument("--limit", type=int, default=DEFAULT_PAGE_SIZE,
                       help=f"Page size (default: {DEFAULT_PAGE_SIZE})")
    parser.add_argument("--db-path", default=DEFAULT_DB_PATH,
                       help=f"DuckDB path (default: {DEFAULT_DB_PATH})")
    parser.add_argument("--table", default=DEFAULT_TABLE,
                       help=f"Table name (default: {DEFAULT_TABLE})")
    parser.add_argument("--app-token", 
                       help="Socrata app token (overrides SOCRATA_APP_TOKEN env)")
    parser.add_argument("--start-offset", type=int, default=0,
                       help="Starting offset for pagination (for resuming downloads)")
    
    args = parser.parse_args()
    
    # Validate arguments
    if args.limit < 1 or args.limit > 50000:
        logger.error("Limit must be between 1 and 50,000")
        sys.exit(1)
    
    try:
        since_dt = datetime.fromisoformat(args.since.replace('Z', '+00:00'))
        until_dt = datetime.fromisoformat(args.until.replace('Z', '+00:00'))
        if since_dt > until_dt:
            logger.error("Since date must be <= until date")
            sys.exit(1)
    except ValueError as e:
        logger.error(f"Invalid date format: {e}")
        sys.exit(1)
    
    # Setup headers
    headers = {}
    app_token = args.app_token or os.getenv('SOCRATA_APP_TOKEN')
    if app_token:
        headers['X-App-Token'] = app_token
        logger.info("Using Socrata app token")
    
    # Connect to database
    logger.info(f"Connecting to database: {args.db_path}")
    conn = connect_duckdb(args.db_path)
    
    # Ensure table exists
    ensure_table(conn, args.table)
    
    # Fetch data in pages
    offset = args.start_offset
    total_fetched = 0
    page_num = (offset // args.limit) + 1
    
    logger.info(f"Starting data fetch: {args.since} to {args.until}")
    if offset > 0:
        logger.info(f"Resuming from offset {offset:,} (page {page_num})")
    
    while True:
        url = build_soql_url(SELECT_COLUMNS, args.since, args.until, args.limit, offset)
        
        logger.info(f"Fetching page {page_num} (offset {offset:,})")
        
        try:
            # Fetch data
            csv_data = http_get_csv(url, headers)
            
            # Parse to DataFrame
            df = parse_csv_to_df(csv_data)
            
            if df.empty:
                logger.info("No more data - fetch complete")
                break
            
            # Upsert to database
            upsert_batch(conn, df, args.table)
            
            rows_in_page = len(df)
            total_fetched += rows_in_page
            
            logger.info(f"Page {page_num}: {rows_in_page:,} rows | Total: {total_fetched:,}")
            
            # Check if we got a full page
            if rows_in_page < args.limit:
                logger.info("Partial page received - fetch complete")
                break
            
            # Prepare for next page
            offset += args.limit
            page_num += 1
            
            # Small delay to be respectful to API - increased for stability
            time.sleep(0.8)
            
        except Exception as e:
            logger.error(f"Error processing page {page_num}: {e}")
            raise
    
    # Create indexes
    logger.info("Creating indexes...")
    create_indexes(conn, args.table)
    
    # Print summary
    print_summary(conn, args.table)
    
    # Close connection
    conn.close()
    logger.info("Data fetch completed successfully!")

if __name__ == "__main__":
    main()

# Example queries to run after loading data:
"""
-- Connect to DuckDB and run these queries:

-- 1) Latest top complaint types:
SELECT complaint_type, COUNT(*) c 
FROM nyc311_2024_2025 
GROUP BY 1 
ORDER BY c DESC 
LIMIT 10;

-- 2) Median closure lag (hours) by borough:
SELECT borough, MEDIAN(EXTRACT('epoch' FROM (closed_date - created_date))/3600.0) AS median_hours
FROM nyc311_2024_2025 
WHERE closed_date IS NOT NULL 
GROUP BY 1 
ORDER BY 2;

-- 3) Monthly request volume:
SELECT DATE_TRUNC('month', created_date) as month, COUNT(*) as requests
FROM nyc311_2024_2025 
GROUP BY 1 
ORDER BY 1;

-- 4) Open vs closed requests by borough:
SELECT borough, status, COUNT(*) as count
FROM nyc311_2024_2025 
GROUP BY 1, 2 
ORDER BY 1, 3 DESC;

-- 5) Geographic distribution (with coordinates):
SELECT borough, COUNT(*) as total,
       COUNT(latitude) as with_coords,
       ROUND(COUNT(latitude) * 100.0 / COUNT(*), 1) as coord_pct
FROM nyc311_2024_2025 
GROUP BY 1 
ORDER BY 2 DESC;
"""